ðŸ“Š Metrics Agent Design â€“ cncpick2

This document outlines the proposed functionality and outputs of the optional metrics_agent, which calculates system-level performance indicators in real time.

â¸»

ðŸŽ¯ Purpose
	â€¢	Measure efficiency and reliability of the system
	â€¢	Provide quantitative feedback during test runs
	â€¢	Aggregate real-time counters and time-based metrics

â¸»

ðŸ“¥ Inputs

Listens to events published by other agents:
	â€¢	object_spawned
	â€¢	object_detected
	â€¢	object_assigned
	â€¢	object_picked
	â€¢	object_dropped
	â€¢	object_missed
	â€¢	unassigned_good_object

â¸»

ðŸ“ˆ Core Metrics

Metric	Calculation Logic
Total Objects Spawned	Count of object_spawned
Detection Rate	detected / spawned
Assignment Rate	assigned / detected
Pickup Success Rate	picked / assigned
Drop Accuracy	Valid dropped matching assigned
Missed Pickups	Count of object_missed
Unassigned Good Objects	Count of unassigned_good_object
Avg. Time: Detect â†’ Assign	Timestamp delta
Avg. Time: Assign â†’ Pickup	Timestamp delta


â¸»

ðŸ”„ Output Events

{
  "event": "metric_report",
  "agent": "metrics_agent",
  "pickup_success_rate": 0.92,
  "missed_pickups": 1,
  "unassigned_good_objects": 3,
  "timestamp": 1234567890.0
}

Also writes tests/logs/metrics_summary.json at end of run.

â¸»

ðŸ§ª Runtime Behavior
	â€¢	Updates metrics every 1â€“2 seconds
	â€¢	Provides snapshot views for dashboards
	â€¢	Emits summary at end of test if sim_mode: true

â¸»

ðŸ“Œ Summary

The metrics_agent enables clear visibility into the internal health and performance of the system. It supports dashboards, developer debugging, and regression tracking across test scenarios.