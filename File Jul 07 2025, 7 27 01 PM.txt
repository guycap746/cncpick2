ğŸ‘ï¸ Vision Depth & Lane Mapping Strategy â€“ cncpick2

This document outlines how the vision system processes RGB + depth data to assign objects to lanes and determine accurate Z pickup height.

â¸»

ğŸ§  Purpose
	â€¢	Detect green objects in live/simulated frames
	â€¢	Extract object centroid, area, and depth
	â€¢	Classify lane by image-space X mapping

â¸»

ğŸ§® Input
	â€¢	RGB frame from camera (sim or real)
	â€¢	Depth map aligned to RGB (from OAK-D or simulator)

â¸»

ğŸ§¾ Detection Output Fields

{
  "event": "object_detected",
  "agent": "vision_agent",
  "object_id": "abc123",
  "lane": 2,
  "type": "green",
  "area": 1450,
  "depth_mm": 78.5,
  "height_mm": 35.0,
  "bbox": [x, y, w, h],
  "timestamp": 1234567890.0
}


â¸»

ğŸ—ºï¸ Lane Mapping
	â€¢	Performed using a lane_mapper JSON config:

{
  "lanes": {
    "0": {"x_min": 0, "x_max": 160},
    "1": {"x_min": 160, "x_max": 320},
    "2": {"x_min": 320, "x_max": 480},
    "3": {"x_min": 480, "x_max": 640}
  }
}

	â€¢	Mapped by centroid X coordinate in pixel space

â¸»

ğŸ“ Depth & Height
	â€¢	Depth = raw distance from camera
	â€¢	Height = object top surface above belt
	â€¢	Formula:

height_mm = belt_z_reference - object_depth_z

	â€¢	belt_z_reference defined per camera setup

â¸»

ğŸ§ª Simulation
	â€¢	Simulated frames provide depth per object
	â€¢	Same pipeline processes both real and sim
	â€¢	Ensures parity across test and hardware deployment

â¸»

ğŸ“Œ Summary

The vision system is responsible for translating raw camera data into actionable metadata for scoring and motion agents. Lane assignment and object height are critical for accurate pickup.